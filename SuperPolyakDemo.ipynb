{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing the $\\ell_1$ norm with SuperPolyak optimizer class\n",
    "\n",
    "In this cell, we use the SuperPolyak optimizer class to optimize the l1 norm:\n",
    "$$\n",
    "f(x) = \\|x\\|_1.\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A pytorch compatible version of the above code.\n",
    "import AlternatingProjections\n",
    "import util\n",
    "import numpy as np\n",
    "import torch\n",
    "import SuperPolyak\n",
    "\n",
    "# Problem dimension\n",
    "d = 10\n",
    "# Number of Equations:\n",
    "max_elts = d  # try 1, 2, 3, d.\n",
    "# The superlinear contraction exponent\n",
    "eta_est = 1.5  # try .1, .5, 1, 1.5, 2\n",
    "# The param.\n",
    "x = torch.randn(d, requires_grad=True, dtype=torch.double)\n",
    "\n",
    "# Definition of l1 norm.\n",
    "def f():\n",
    "    return torch.sum(abs(x))\n",
    "\n",
    "\n",
    "# Define the Superpolyak Optimizer.\n",
    "optimizer = SuperPolyak.SuperPolyak([x], max_elts=max_elts, eta_est=eta_est)\n",
    "# Closure function to allow us to call backward.\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = f()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "current_iter = 0\n",
    "while closure().item() > 1e-20 and current_iter < 100:\n",
    "    loss, bundle_index = optimizer.step(closure)\n",
    "    print(\"f(y)\", closure().item())\n",
    "    # the number of equations that were used: bundle_index <= max_elts.\n",
    "    print(\"Bundle index\", bundle_index)\n",
    "    current_iter += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing a small neural network with SuperPolyak\n",
    "\n",
    "We fit a small 2 layer neural network $F$ to random gaussian data $(x,y)$. The loss function is\n",
    "$$\n",
    "f(x) = \\frac{1}{n}\\|F(x) - y\\|_1\n",
    "$$\n",
    "You can change $\\ell_1$ to an $\\ell_p$ norm and expect the same performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitting a small neural network with pytorch.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import SuperPolyak\n",
    "\n",
    "input_size = 100\n",
    "hidden_unit_size = 2000\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_unit_size, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(hidden_unit_size, 1, dtype=torch.double)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# Let d be the number of parameters in net\n",
    "d = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"number of parameters\", d)\n",
    "# Number of equations\n",
    "max_elts = int(200)\n",
    "print(\"bundle size\", max_elts)\n",
    "# The superlinear contraction exponent\n",
    "eta_est = 0.5\n",
    "\n",
    "# Fake training data\n",
    "x = torch.randn(1000, input_size, dtype=torch.double)\n",
    "y = net(x).detach().clone().requires_grad_(False)\n",
    "\n",
    "# Reset the parameters to a random initialization.\n",
    "net = Net()\n",
    "\n",
    "# Define the l1 loss.\n",
    "def loss_function():\n",
    "    return sum(torch.abs(net(x) - y))\n",
    "\n",
    "\n",
    "# a closure function to allow us to call backward\n",
    "max_oracle_calls = 10000\n",
    "params = list(net.parameters())\n",
    "# Set the linsys_solver to be QR based.\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.QR\n",
    "\n",
    "# Define the Superpolyak Optimizer.\n",
    "optimizer = SuperPolyak.SuperPolyak(\n",
    "    params, max_elts=max_elts, eta_est=eta_est, linsys_solver=linsys_solver\n",
    ")\n",
    "\n",
    "\n",
    "# Define the closure function.\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Bookkeeping\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\n",
    "        \"Iteration: \",\n",
    "        cumulative_oracle_calls[-1],\n",
    "        \", Loss: \",\n",
    "        closure().item(),\n",
    "        \", Bundle_exit_step \",  # the number of equations in the linear system.\n",
    "        bundle_idx,\n",
    "    )\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fitting a 1-layer neural network with max pooling via the problems.py class\n",
    "\n",
    "We have included a module called `problems.py` that allows you to easily define several \"random\" problem instances. In the remaining examples, we're going to use this module to define several problems.\n",
    "\n",
    "In this first example, we generate a 1 layer neural network with max pooling, $F$, and fit it to random Gaussian data $(x, y)$, where $y = F(x)$. The loss function is\n",
    "$$\n",
    "f(x) = \\frac{1}{n}\\|F(x) - y\\|_1\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import problems\n",
    "\n",
    "d = 1000\n",
    "# Number of hidden units\n",
    "k = 10\n",
    "m = 4 * d * k\n",
    "# Import a random problem instance.\n",
    "z = problems.MaxAffineRegressionProblem(m=m, d=d, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "max_elts = 40\n",
    "eta_est = 2\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.QR\n",
    "optimizer = SuperPolyak.SuperPolyak(param, max_elts=max_elts, eta_est=eta_est)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "# A short loop that applies the SuperPolyak subgradient method to the loss function\n",
    "max_oracle_calls = 1000\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\n",
    "        \"Iteration: {}, Loss: {}, Bundle idx: {}\".format(\n",
    "            cumulative_oracle_calls[-1], loss, bundle_idx\n",
    "        )\n",
    "    )\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d * k))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An illustration of coupling SuperPolyak with the Polyak subgradient method\n",
    "\n",
    "Now we're going to demo how `util.superpolyak_coupled_with_fallback` can be used to couple SuperPolyak with a generic \"fallback\" method. One should think of the fallback method as more reliable than SuperPolyak, but potentially slow. Thus, when a SuperPolyak \"fails,\" we switch to the fallback method for a bit, and then eventually switch back to the fallback method if we need to.\n",
    "\n",
    "In this example, we couple SuperPolyak with the \"Polyak\" subgradient method, which is just SuperPolyak with `max_elts = 1`. We'll use the same problem instance as before. You'll see that the SuperPolyak method never fails in this example, so we never switch to the fallback method. This is not always the case, as we will see below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "\n",
    "d = 1000\n",
    "k = 2\n",
    "m = 4 * d * k\n",
    "z = problems.MaxAffineRegressionProblem(m=m, d=d, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "max_elts = 40\n",
    "eta_est = 2\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.QR\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(\n",
    "    param, max_elts=max_elts, eta_est=eta_est\n",
    ")\n",
    "polyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=1, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "max_inner_iter = 10\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=closure,\n",
    "    fallback_closure=closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=polyak_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Max affine regression with {} parameters\".format(d * 2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Illustration of using the prox-gradient method as a fallback method for Lasso\n",
    "\n",
    "This example illustrates a case where the fallback method does a lot of work.\n",
    "\n",
    "We're interested in solving the (convex) Lasso problem\n",
    "$$\n",
    "\\min_{x\\in \\mathbb{R}^d} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1.\n",
    "$$\n",
    "We generate a random instance of the problem, where\n",
    "- $A \\in \\mathbb{R}^{m \\times d}$ is a random Gaussian matrix.\n",
    "- $b = A \\bar x$\n",
    "- $\\bar x$ is a random $k$ sparse vector.\n",
    "\n",
    "We choose $\\lambda$ specifically so that $\\bar x$ is a solution to the problem. The proximal gradient algorithm is a standard method for Lasso problem class. While there exist faster methods for the problem class, we will use this as our fallback.\n",
    "\n",
    "For our loss function $f$, we use the following nonsmooth, nonconvex loss function:\n",
    "\n",
    "$$\n",
    "f(x) = \\|x - \\mathrm{prox}_{\\lambda \\|\\cdot\\|_1} (x - \\tau A^T(Ax - b)))\\|_2\n",
    "$$\n",
    "\n",
    "The loss is motivated by the well-known property that the solutions to the Lasso problem are precisely the fixed-points of the proximal gradient operator:\n",
    "$$\n",
    "T(x) := \\mathrm{prox}_{\\lambda \\|\\cdot\\|_1} (x - \\tau A^T(Ax - b))).\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import proxgradient\n",
    "import torch\n",
    "\n",
    "# The dimension\n",
    "d = 1000\n",
    "# the sparsity level\n",
    "k = 2\n",
    "# The number of linear measuremennts\n",
    "m = 4 * k\n",
    "z = problems.LassoProblem(m=m, d=d, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "max_elts = 40\n",
    "eta_est = 2\n",
    "linsys_solver = (\n",
    "    SuperPolyak.BundleLinearSystemSolver.LSMR\n",
    ")  # QR becomes unstable for this problem\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(\n",
    "    param, max_elts=max_elts, eta_est=eta_est, linsys_solver=linsys_solver\n",
    ")\n",
    "proxgradient_optimizer = proxgradient.ProxGradient(\n",
    "    param, proxs=[z.prox], lr=z.prox_step\n",
    ")\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def fallback_closure():\n",
    "    proxgradient_optimizer.zero_grad()\n",
    "    loss = 0.5 * torch.sum(torch.square(z.A @ init - z.y))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=fallback_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=proxgradient_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Lasso problem with {} parameters\".format(d * 2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase retrieval on distance functions with Alternating Projections Fallback\n",
    "\n",
    "This example illustrates how SuperPolyak can accelerate the alternating projections method for a nonconvex problem.\n",
    "\n",
    "We focus on *phase retrieval*, where the goal is to recover a complex-valued signal $\\bar{x} \\in \\mathbb{C}^d$ from a set of phaseless measurements:\n",
    "\n",
    "$$\n",
    "y_i = |\\langle a_i, \\bar{x}\\rangle|, \\; \\text{for $i = 1, \\dots, m$}.\n",
    "$$\n",
    "\n",
    "To recover $\\bar{x}$, we consider the feasibility formulation\n",
    "\n",
    "$$\n",
    "\\text{find $\\hat{y} \\in \\mathcal{Y} \\cap \\mathrm{Range}(A)$}, \\quad \\text{where} \\quad\n",
    "\\mathcal{Y} := \\{u \\in \\mathbb{C}^m \\mid |u| = y\\},\n",
    "$$\n",
    "\n",
    "where $A$ is the matrix whose $i$-th row is $a_i^*$, and estimate $\\bar{x}$ using $A^{\\dagger} \\hat{y}$. A standard method for solving this problem is the **alternating projections** method:\n",
    "\n",
    "$$\n",
    "y_{k+1} := \\mathrm{proj}_{\\mathrm{range}(A)}\\left( \\mathrm{proj}_{\\mathcal{Y}}(y_k) \\right), \\; \\; k = 0, 1, \\dots,\n",
    "$$\n",
    "\n",
    "where each projections is available in closed form. Using the method as a fallback, we apply SuperPolyak to the nonnegative loss function\n",
    "\n",
    "$$\n",
    "f(y) := \\mathrm{dist}(y, \\mathcal{Y}) + \\mathrm{dist}(y, \\mathrm{range}(A)).\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import AlternatingProjections\n",
    "import torch\n",
    "\n",
    "d = 100\n",
    "m = 8 * d\n",
    "z = problems.PhaseRetrievalProblem(m=m, d=d)\n",
    "init = z.initializer_altproj(1.0)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=10, eta_est=0.5)\n",
    "projs = [z.alternating_projections_step()]\n",
    "AlternatingProjections_optimizer = AlternatingProjections.AlternatingProjections(\n",
    "    param, projs=projs\n",
    ")\n",
    "loss_function = z.loss_altproj()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=superpolyak_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=AlternatingProjections_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Complex phase retrieval problem problem with {} parameters and {} measurements.\".format(\n",
    "        d * 2, m\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase retrieval with a generative prior\n",
    "This example is a demonstration of SuperPolyak for solving real phase retrieval under a *generative prior*.\n",
    "Here, we wish to recover a real-valued $\\bar{x} \\in \\mathbb{R}^d$ from a number of measurements\n",
    "\n",
    "$$\n",
    "y_i = |\\langle a_i, \\bar{x} \\rangle|, \\; \\; \\text{for $i = 1, \\dots, m$}.\n",
    "$$\n",
    "\n",
    "The problem is ill-posed in general when the number of measurements $m$ is less than the ambient dimension $d$. To enable recovery, it is typically assumed that $\\bar{x}$ lies in a \"low-complexity\" set (e.g., the set of $k$-sparse vectors for some $k < d$).\n",
    "\n",
    "Here, we impose a **generative prior** on $\\bar{x}$. In particular, we assume that\n",
    "\n",
    "$$\n",
    "\\bar{x} = G(\\bar{z}), \\quad \\bar{z} \\in \\mathbb{R}^k,\n",
    "$$\n",
    "\n",
    "where $G: \\mathbb{R}^k \\to \\mathbb{R}^d$ is a ReLU network with fixed weights and $k \\ll d$. To recover $\\bar{x}$, we optimize over the space of latent codes $z$:\n",
    "\n",
    "$$\n",
    "\\min_{z \\in \\mathbb{R}^k} F(z) := \\frac{1}{m} \\sum_{i = 1}^m |\\langle a_i, G(z)\\rangle - y_i|.\n",
    "$$\n",
    "\n",
    "As in previous examples, we couple SuperPolyak with the Polyak subgradient method for this instance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import torch\n",
    "\n",
    "latent_dimension = 200\n",
    "d = 5000\n",
    "m = min([5 * latent_dimension, 5 * d])\n",
    "z = problems.GenerativePhaseRetrievalProblem(\n",
    "    m=m, d=d, latent_dimension=latent_dimension\n",
    ")\n",
    "init = z.initializer(0.5)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=20, eta_est=2)\n",
    "polyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=1, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def polyak_closure():\n",
    "    polyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 50\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=polyak_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=polyak_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Phase retrieval with a generative prior.\\n Signal dimension {}\\n Number of Latent parameters: {}\\n Number of measurements {}\".format(\n",
    "        latent_dimension, d, m\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression with $\\ell_2$ regularization: coupling gradient descent and SuperPolyak on Norm of gradient.\n",
    "\n",
    "In this example, we consider an $\\ell_2$ regularized logistic regression problem on random Gaussian data. At a high-level, the problem is\n",
    "\n",
    "$$\n",
    "\\min  l(x)\n",
    "$$\n",
    "\n",
    "where $l$ is a smooth and strongly convex problem. As our fallback method, we will use the gradient descent algorithm, which converges linearly on this problem.\n",
    "\n",
    "For our loss function $f$, we simply use the norm of the gradient of $l$:\n",
    "$$\n",
    "f(x) = \\|\\nabla l(x)\\|.\n",
    "$$\n",
    "For this problem class, $f(x) = 0$ precisely when $x$ minimizes $l$.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import torch\n",
    "\n",
    "# add time to the imports\n",
    "import time\n",
    "\n",
    "d = 100000\n",
    "m = 1000\n",
    "l2_penalty = .0001\n",
    "z = problems.LogisticRegressionProblem(m=m, d=d, l2_penalty=l2_penalty)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.QR\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(\n",
    "    param, max_elts=2, eta_est=2, linsys_solver=linsys_solver\n",
    ")\n",
    "sgd_optimizer = torch.optim.SGD(param, lr=z.lr)\n",
    "loss_function = z.norm_grad()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sgd_closure():\n",
    "    sgd_optimizer.zero_grad()\n",
    "    loss = z.loss()(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "# start timer\n",
    "start = time.time()\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=sgd_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=sgd_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-16,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "# end timer\n",
    "end = time.time()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Logistic regression problem with {} parameters\".format(d))\n",
    "# modify the title so it also includes the number of samples `m` and the l2 penalty parameter\n",
    "plt.title(\n",
    "    \"Logistic regression problem with {} parameters, {} samples, and l2 penalty {}\".format(\n",
    "        d, m, z.l2_penalty\n",
    "    )\n",
    ")\n",
    "plt.show()\n",
    "print(\"Elapsed time: {} seconds\".format(end - start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Newton's method on $\\ell_2$ regularized logistic regression problem\n",
    "\n",
    "Here we demonstrate a Newton algorithm on the same problem as in the previous cell. In our implementation, we use a CG type algorithm to solve the linear system."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import problems\n",
    "import torch\n",
    "import time\n",
    "import newtoncg\n",
    "\n",
    "d = 100000\n",
    "m = 1000\n",
    "l2_penalty = .0001\n",
    "z = problems.LogisticRegressionProblem(m=m, d=d, l2_penalty=l2_penalty)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "Newton_optimizer = newtoncg.NewtonCG(param)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def Newton_closure(x):\n",
    "    Newton_optimizer.zero_grad()\n",
    "    loss = loss_function(x)\n",
    "    # loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def grad_norm_closure():\n",
    "    Newton_optimizer.zero_grad()\n",
    "    loss = z.norm_grad()(init)\n",
    "    # loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "max_oracle_calls = 100\n",
    "gap = [Newton_closure(init).item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "# start timer\n",
    "start = time.time()\n",
    "while (\n",
    "    grad_norm_closure().item() > 1e-16\n",
    "    and cumulative_oracle_calls[-1] < max_oracle_calls\n",
    "):\n",
    "    Newton_optimizer.step(Newton_closure)\n",
    "    print(\n",
    "        \"Iteration: \",\n",
    "        cumulative_oracle_calls[-1],\n",
    "        \", Loss: \",\n",
    "        grad_norm_closure().item(),\n",
    "    )\n",
    "    cumulative_oracle_calls.append(1 + cumulative_oracle_calls[-1])\n",
    "    gap.append(grad_norm_closure().item())\n",
    "# end timer\n",
    "end = time.time()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Logistic regression problem with {} parameters, {} samples, and l2 penalty {}\".format(\n",
    "        d, m, z.l2_penalty\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Elapsed time: {} seconds\".format(end - start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Comparing SuperPolyak and Newton's method on $\\ell_2$ regularized logistic regression problem\n",
    "\n",
    "Now we compare the performance of SuperPolyak and Newton's method on the same problem as in the previous cell. In our implementation, we use a CG type algorithm to solve the Newton system.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This cell compares newtoncg to SuperPolyak on the same problem as in the previous cell.\n",
    "# It will run the code for\n",
    "## m = 1000\n",
    "## d in [1000, 10000, 100000]\n",
    "## l2_penalty =.0001\n",
    "# It will plot the number of oracle calls vs the norm of the gradient of the loss function.\n",
    "# It will also print the time taken for each method to run for each problem size `d`.\n",
    "## The time output will be in the form of a table.\n",
    "## The first column is the problem size `d`.\n",
    "## The second column is time taken by SuperPolyak.\n",
    "## The third column is time taken by NewtonCG.\n",
    "import problems\n",
    "import torch\n",
    "import time\n",
    "import newtoncg\n",
    "import SuperPolyak\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "\n",
    "# define m and l2_penalty\n",
    "m = 1000\n",
    "l2_penalty = .0001\n",
    "\n",
    "# define the problem sizes\n",
    "d_list = [1000, 10000, 100000]\n",
    "\n",
    "# define a list to store the time taken by SuperPolyak\n",
    "superpolyak_time = []\n",
    "\n",
    "# define a list to store the time taken by NewtonCG\n",
    "newtoncg_time = []\n",
    "\n",
    "# define a list to store the number of oracle calls for SuperPolyak\n",
    "superpolyak_oracle_calls = []\n",
    "\n",
    "# define a list to store the number of oracle calls for NewtonCG\n",
    "newtoncg_oracle_calls = []\n",
    "\n",
    "# define a list to store the norm of the gradient of the loss function for SuperPolyak\n",
    "superpolyak_grad_norm = []\n",
    "\n",
    "# define a list to store the norm of the gradient of the loss function for NewtonCG\n",
    "newtoncg_grad_norm = []\n",
    "\n",
    "tol = 1e-16\n",
    "init_distance = 1\n",
    "# for each problem size, run SuperPolyak and NewtonCG\n",
    "for d in d_list:\n",
    "    # define the problem\n",
    "    z = problems.LogisticRegressionProblem(m=m, d=d, l2_penalty=l2_penalty)\n",
    "    # define the initial point\n",
    "    init = z.initializer(init_distance)\n",
    "    # define the parameter\n",
    "    param = [init]\n",
    "    # define the linear system solver\n",
    "    linsys_solver = SuperPolyak.BundleLinearSystemSolver.QR\n",
    "    # define the SuperPolyak optimizer\n",
    "    superpolyak_optimizer = SuperPolyak.SuperPolyak(\n",
    "        param, max_elts=2, eta_est=2, linsys_solver=linsys_solver\n",
    "    )\n",
    "    # define the SGD optimizer\n",
    "    sgd_optimizer = torch.optim.SGD(param, lr=z.lr)\n",
    "    loss_function = z.norm_grad()\n",
    "\n",
    "    # define the superpolyak closure\n",
    "    def superpolyak_closure():\n",
    "        superpolyak_optimizer.zero_grad()\n",
    "        loss = loss_function(init)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    # define the sgd closure\n",
    "    def sgd_closure():\n",
    "        sgd_optimizer.zero_grad()\n",
    "        loss = z.loss(init)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "\n",
    "    # define the oracle calls and loss list for SuperPolyak\n",
    "    oracle_calls = [0]\n",
    "    loss_list = [superpolyak_closure().item()]\n",
    "\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "    max_inner_iter = 100\n",
    "    max_outer_iter = 100\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "    oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "        superpolyak_closure=superpolyak_closure,\n",
    "        fallback_closure=sgd_closure,\n",
    "        superpolyak_optimizer=superpolyak_optimizer,\n",
    "        fallback_optimizer=sgd_optimizer,\n",
    "        max_inner_iter=max_inner_iter,\n",
    "        tol=tol,\n",
    "        max_outer_iter=max_outer_iter,\n",
    "        verbose=True,\n",
    "    )\n",
    "    # end timer\n",
    "    end = time.time()\n",
    "\n",
    "    # store the time taken by SuperPolyak\n",
    "    superpolyak_time.append(end - start)\n",
    "\n",
    "    # store the number of oracle calls for SuperPolyak\n",
    "    superpolyak_oracle_calls.append(oracle_calls)\n",
    "\n",
    "    # store the norm of the gradient of the loss function for SuperPolyak\n",
    "    superpolyak_grad_norm.append(loss_list)\n",
    "\n",
    "    # reinitialize the parameter\n",
    "    init = z.initializer(init_distance)\n",
    "    param = [init]\n",
    "    # define the newton optimizer\n",
    "    Newton_optimizer = newtoncg.NewtonCG(param)\n",
    "    def grad_norm_closure():\n",
    "        Newton_optimizer.zero_grad()\n",
    "        loss = z.norm_grad()(init)\n",
    "        # loss.backward()\n",
    "        return loss\n",
    "    # define the newton closure\n",
    "    def Newton_closure(x):\n",
    "        Newton_optimizer.zero_grad()\n",
    "        loss = z.loss()(x)\n",
    "        # loss.backward()\n",
    "        return loss\n",
    "    # define the oracle calls and loss list for NewtonCG\n",
    "    oracle_calls_newton = [0]\n",
    "    loss_list_newton = [grad_norm_closure().item()]\n",
    "    # define the loss function, which is the norm of the gradient of the loss function\n",
    "    # start timer\n",
    "    max_oracle_calls = 10\n",
    "    start = time.time()\n",
    "    while (\n",
    "        loss_list_newton[-1] > tol\n",
    "        and oracle_calls_newton[-1] < max_oracle_calls\n",
    "    ):\n",
    "        Newton_optimizer.step(Newton_closure)\n",
    "        oracle_calls_newton.append(1 + oracle_calls_newton[-1])\n",
    "        loss_list_newton.append(grad_norm_closure().item())\n",
    "\n",
    "    # end timer\n",
    "    end = time.time()\n",
    "\n",
    "    # store the time taken by NewtonCG\n",
    "    newtoncg_time.append(end - start)\n",
    "\n",
    "    # store the number of oracle calls for NewtonCG\n",
    "    newtoncg_oracle_calls.append(oracle_calls_newton)\n",
    "\n",
    "    # store the norm of the gradient of the loss function for NewtonCG\n",
    "    newtoncg_grad_norm.append(loss_list_newton)\n",
    "\n",
    "    print(\"d = {}, SuperPolyak time = {}, NewtonCG time = {}\".format(d, superpolyak_time[-1], newtoncg_time[-1]))\n",
    "\n",
    "# Now for each problem size, plot the number of oracle calls vs the norm of the gradient of the loss function.\n",
    "## add a legend to the plot to indicate which method is which and which problem size is which.\n",
    "## add a title to the plot to indicate the value of m and l2_penalty.\n",
    "## add a label to the x-axis to indicate what it is.\n",
    "## add a label to the y-axis to indicate what it is.\n",
    "\n",
    "\n",
    "# for each problem size plot the number of oracle calls vs the norm of the gradient of the loss function for SuperPolyak\n",
    "for d in d_list:\n",
    "    plt.semilogy(superpolyak_oracle_calls[d_list.index(d)], superpolyak_grad_norm[d_list.index(d)], label=\"SuperPolyak, d = {}\".format(d))\n",
    "\n",
    "# for each problem size plot the number of oracle calls vs the norm of the gradient of the loss function for NewtonCG\n",
    "for d in d_list:\n",
    "    plt.semilogy(newtoncg_oracle_calls[d_list.index(d)], newtoncg_grad_norm[d_list.index(d)], label=\"NewtonCG, d = {}\".format(d))\n",
    "\n",
    "# add a legend to the plot to indicate which method is which and which problem size is which\n",
    "plt.legend()\n",
    "# move the legend to the top right of the plot\n",
    "plt.legend(loc=\"upper right\")\n",
    "# add a title to the plot to indicate the value of m and l2_penalty and the fact that this is logistic regression\n",
    "plt.title(\"Logistic Regression, m = {}, l2_penalty = {}\".format(m, l2_penalty))\n",
    "# add a label to the x-axis to indicate what it is\n",
    "plt.xlabel(\"Number of Oracle Calls\")\n",
    "# add a label to the y-axis to indicate what it is\n",
    "plt.ylabel(\"Norm of the Gradient of the Loss Function\")\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n",
    "# now print a table of the number the time taken for each problem size for SuperPolyak and NewtonCG\n",
    "print(\"d\\tSuperPolyak time (s)\\tNewtonCG time (s)\")\n",
    "# make sure the table is aligned\n",
    "print(\"-------------------------------------------------\")\n",
    "# for each problem size print the time taken for SuperPolyak and NewtonCG\n",
    "for d in d_list:\n",
    "    print(\"{}\\t{}\\t{}\".format(d, superpolyak_time[d_list.index(d)], newtoncg_time[d_list.index(d)]))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}