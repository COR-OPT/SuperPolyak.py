{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing the $\\ell_1$ norm with SuperPolyak optimizer class\n",
    "\n",
    "In this cell, we use the SuperPolyak optimizer class to optimize the l1 norm:\n",
    "$$\n",
    "f(x) = \\|x\\|_1.\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A pytorch compatible version of the above code.\n",
    "import AlternatingProjections\n",
    "import util\n",
    "import numpy as np\n",
    "import torch\n",
    "import SuperPolyak\n",
    "\n",
    "# Problem dimension\n",
    "d = 10\n",
    "# Number of Equations:\n",
    "max_elts = d  # try 1, 2, 3, d.\n",
    "# The superlinear contraction exponent\n",
    "eta_est = 1.5  # try .1, .5, 1, 1.5, 2\n",
    "# The param.\n",
    "x = torch.randn(d, requires_grad=True, dtype=torch.double)\n",
    "\n",
    "# Definition of l1 norm.\n",
    "def f():\n",
    "    return torch.sum(abs(x))\n",
    "\n",
    "\n",
    "# Define the Superpolyak Optimizer.\n",
    "optimizer = SuperPolyak.SuperPolyak([x], max_elts=max_elts, eta_est=eta_est)\n",
    "# Closure function to allow us to call backward.\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = f()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "current_iter = 0\n",
    "while closure().item() > 1e-20 and current_iter < 100:\n",
    "    loss, bundle_index = optimizer.step(closure)\n",
    "    print(\"f(y)\", closure().item())\n",
    "    # the number of equations that were used: bundle_index <= max_elts.\n",
    "    print(\"Bundle index\", bundle_index)\n",
    "    current_iter += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing a small neural network with SuperPolyak\n",
    "\n",
    "We fit a small 2 layer neural network $F$ to random gaussian data $(x,y)$. The loss function is\n",
    "$$\n",
    "f(x) = \\frac{1}{n}\\|F(x) - y\\|_1\n",
    "$$\n",
    "You can change $\\ell_1$ to an $\\ell_p$ norm and expect the same performance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitting a small neural network with pytorch.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import SuperPolyak\n",
    "\n",
    "input_size = 100\n",
    "hidden_unit_size = 2000\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_unit_size, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(hidden_unit_size, 1, dtype=torch.double)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# Let d be the number of parameters in net\n",
    "d = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"number of parameters\", d)\n",
    "# Number of equations\n",
    "max_elts = int(200)\n",
    "print(\"bundle size\", max_elts)\n",
    "# The superlinear contraction exponent\n",
    "eta_est = 0.5\n",
    "\n",
    "# Fake training data\n",
    "x = torch.randn(1000, input_size, dtype=torch.double)\n",
    "y = net(x).detach().clone().requires_grad_(False)\n",
    "\n",
    "# Reset the parameters to a random initialization.\n",
    "net = Net()\n",
    "\n",
    "# Define the l1 loss.\n",
    "def loss_function():\n",
    "    return sum(torch.abs(net(x) - y))\n",
    "\n",
    "\n",
    "# a closure function to allow us to call backward\n",
    "max_oracle_calls = 10000\n",
    "params = list(net.parameters())\n",
    "# Set the linsys_solver to be QR based.\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.QR\n",
    "\n",
    "# Define the Superpolyak Optimizer.\n",
    "optimizer = SuperPolyak.SuperPolyak(\n",
    "    params, max_elts=max_elts, eta_est=eta_est, linsys_solver=linsys_solver\n",
    ")\n",
    "\n",
    "\n",
    "# Define the closure function.\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Bookkeeping\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\n",
    "        \"Iteration: \",\n",
    "        cumulative_oracle_calls[-1],\n",
    "        \", Loss: \",\n",
    "        closure().item(),\n",
    "        \", Bundle_exit_step \",  # the number of equations in the linear system.\n",
    "        bundle_idx,\n",
    "    )\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Fitting a 1-layer neural network with max pooling via the problems.py class\n",
    "\n",
    "We have included a module called `problems.py` that allows you to easily define several \"random\" problem instances. In the remaining examples, we're going to use this module to define several problems.\n",
    "\n",
    "In this first example, we generate a 1 layer neural network with max pooling, $F$, and fit it to random Gaussian data $(x, y)$, where $y = F(x)$. The loss function is\n",
    "$$\n",
    "f(x) = \\frac{1}{n}\\|F(x) - y\\|_1\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import problems\n",
    "\n",
    "d = 1000\n",
    "# Number of hidden units\n",
    "k = 10\n",
    "m = 4 * d * k\n",
    "# Import a random problem instance.\n",
    "z = problems.MaxAffineRegressionProblem(m=m, d=d, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "max_elts = 40\n",
    "eta_est = 2\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.QR\n",
    "optimizer = SuperPolyak.SuperPolyak(param, max_elts=max_elts, eta_est=eta_est)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "# A short loop that applies the SuperPolyak subgradient method to the loss function\n",
    "max_oracle_calls = 1000\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\n",
    "        \"Iteration: {}, Loss: {}, Bundle idx: {}\".format(\n",
    "            cumulative_oracle_calls[-1], loss, bundle_idx\n",
    "        )\n",
    "    )\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d * k))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An illustration of coupling SuperPolyak with the Polyak subgradient method\n",
    "\n",
    "Now we're going to demo how `util.superpolyak_coupled_with_fallback` can be used to couple SuperPolyak with a generic \"fallback\" method. One should think of the fallback method as more reliable than SuperPolyak, but potentially slow. Thus, when a SuperPolyak \"fails,\" we switch to the fallback method for a bit, and then eventually switch back to the fallback method if we need to.\n",
    "\n",
    "In this example, we couple SuperPolyak with the \"Polyak\" subgradient method, which is just SuperPolyak with `max_elts = 1`. We'll use the same problem instance as before. You'll see that the SuperPolyak method never fails in this example, so we never switch to the fallback method. This is not always the case, as we will see below."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "\n",
    "d = 1000\n",
    "k = 2\n",
    "m = 4 * d * k\n",
    "z = problems.MaxAffineRegressionProblem(m=m, d=d, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "max_elts = 40\n",
    "eta_est = 2\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.QR\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(\n",
    "    param, max_elts=max_elts, eta_est=eta_est\n",
    ")\n",
    "polyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=1, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "max_inner_iter = 10\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=closure,\n",
    "    fallback_closure=closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=polyak_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Max affine regression with {} parameters\".format(d * 2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Illustration of using the prox-gradient method as a fallback method for Lasso\n",
    "\n",
    "This example illustrates a case where the fallback method does a lot of work.\n",
    "\n",
    "We're interested in solving the (convex) Lasso problem\n",
    "$$\n",
    "\\min_{x\\in \\mathbb{R}^d} \\frac{1}{2}\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1.\n",
    "$$\n",
    "We generate a random instance of the problem, where\n",
    "- $A \\in \\mathbb{R}^{m \\times d}$ is a random Gaussian matrix.\n",
    "- $b = A \\bar x$\n",
    "- $\\bar x$ is a random $k$ sparse vector.\n",
    "\n",
    "We choose $\\lambda$ specifically so that $\\bar x$ is a solution to the problem. The proximal gradient algorithm is a standard method for Lasso problem class. While there exist faster methods for the problem class, we will use this as our fallback.\n",
    "\n",
    "For our loss function $f$, we use the following nonsmooth, nonconvex loss function:\n",
    "\n",
    "$$\n",
    "f(x) = \\|x - \\mathrm{prox}_{\\lambda \\|\\cdot\\|_1} (x - \\tau A^T(Ax - b)))\\|_2\n",
    "$$\n",
    "\n",
    "The loss is motivated by the well-known property that the solutions to the Lasso problem are precisely the fixed-points of the proximal gradient operator:\n",
    "$$\n",
    "T(x) := \\mathrm{prox}_{\\lambda \\|\\cdot\\|_1} (x - \\tau A^T(Ax - b))).\n",
    "$$\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import proxgradient\n",
    "import torch\n",
    "\n",
    "# The dimension\n",
    "d = 1000\n",
    "# the sparsity level\n",
    "k = 2\n",
    "# The number of linear measuremennts\n",
    "m = 4 * k\n",
    "z = problems.LassoProblem(m=m, d=d, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "max_elts = 40\n",
    "eta_est = 2\n",
    "linsys_solver = (\n",
    "    SuperPolyak.BundleLinearSystemSolver.LSMR\n",
    ")  # QR becomes unstable for this problem\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(\n",
    "    param, max_elts=max_elts, eta_est=eta_est, linsys_solver=linsys_solver\n",
    ")\n",
    "proxgradient_optimizer = proxgradient.ProxGradient(\n",
    "    param, proxs=[z.prox], lr=z.prox_step\n",
    ")\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def fallback_closure():\n",
    "    proxgradient_optimizer.zero_grad()\n",
    "    loss = 0.5 * torch.sum(torch.square(z.A @ init - z.y))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=fallback_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=proxgradient_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Lasso problem with {} parameters\".format(d * 2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase retrieval on distance functions with Alternating Projections Fallback\n",
    "\n",
    "This example illustrates how SuperPolyak can accelerate the alternating projections method for a nonconvex problem.\n",
    "\n",
    "We focus on *phase retrieval*, where the goal is to recover a complex-valued signal $\\bar{x} \\in \\mathbb{C}^d$ from a set of phaseless measurements:\n",
    "\n",
    "$$\n",
    "y_i = |\\langle a_i, \\bar{x}\\rangle|, \\; \\text{for $i = 1, \\dots, m$}.\n",
    "$$\n",
    "\n",
    "To recover $\\bar{x}$, we consider the feasibility formulation\n",
    "\n",
    "$$\n",
    "\\text{find $\\hat{y} \\in \\mathcal{Y} \\cap \\mathrm{Range}(A)$}, \\quad \\text{where} \\quad\n",
    "\\mathcal{Y} := \\{u \\in \\mathbb{C}^m \\mid |u| = y\\},\n",
    "$$\n",
    "\n",
    "where $A$ is the matrix whose $i$-th row is $a_i^*$, and estimate $\\bar{x}$ using $A^{\\dagger} \\hat{y}$. A standard method for solving this problem is the **alternating projections** method:\n",
    "\n",
    "$$\n",
    "y_{k+1} := \\mathrm{proj}_{\\mathrm{range}(A)}\\left( \\mathrm{proj}_{\\mathcal{Y}}(y_k) \\right), \\; \\; k = 0, 1, \\dots,\n",
    "$$\n",
    "\n",
    "where each projections is available in closed form. Using the method as a fallback, we apply SuperPolyak to the nonnegative loss function\n",
    "\n",
    "$$\n",
    "f(y) := \\mathrm{dist}(y, \\mathcal{Y}) + \\mathrm{dist}(y, \\mathrm{range}(A)).\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import AlternatingProjections\n",
    "import torch\n",
    "\n",
    "d = 100\n",
    "m = 8 * d\n",
    "z = problems.PhaseRetrievalProblem(m=m, d=d)\n",
    "init = z.initializer_altproj(1.0)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=10, eta_est=0.5)\n",
    "projs = [z.alternating_projections_step()]\n",
    "AlternatingProjections_optimizer = AlternatingProjections.AlternatingProjections(\n",
    "    param, projs=projs\n",
    ")\n",
    "loss_function = z.loss_altproj()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=superpolyak_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=AlternatingProjections_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Complex phase retrieval problem problem with {} parameters and {} measurements.\".format(\n",
    "        d * 2, m\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase retrieval with a generative prior\n",
    "This example is a demonstration of SuperPolyak for solving real phase retrieval under a *generative prior*.\n",
    "Here, we wish to recover a real-valued $\\bar{x} \\in \\mathbb{R}^d$ from a number of measurements\n",
    "\n",
    "$$\n",
    "y_i = |\\langle a_i, \\bar{x} \\rangle|, \\; \\; \\text{for $i = 1, \\dots, m$}.\n",
    "$$\n",
    "\n",
    "The problem is ill-posed in general when the number of measurements $m$ is less than the ambient dimension $d$. To enable recovery, it is typically assumed that $\\bar{x}$ lies in a \"low-complexity\" set (e.g., the set of $k$-sparse vectors for some $k < d$).\n",
    "\n",
    "Here, we impose a **generative prior** on $\\bar{x}$. In particular, we assume that\n",
    "\n",
    "$$\n",
    "\\bar{x} = G(\\bar{z}), \\quad \\bar{z} \\in \\mathbb{R}^k,\n",
    "$$\n",
    "\n",
    "where $G: \\mathbb{R}^k \\to \\mathbb{R}^d$ is a ReLU network with fixed weights and $k \\ll d$. To recover $\\bar{x}$, we optimize over the space of latent codes $z$:\n",
    "\n",
    "$$\n",
    "\\min_{z \\in \\mathbb{R}^k} F(z) := \\frac{1}{m} \\sum_{i = 1}^m |\\langle a_i, G(z)\\rangle - y_i|.\n",
    "$$\n",
    "\n",
    "As in previous examples, we couple SuperPolyak with the Polyak subgradient method for this instance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import torch\n",
    "\n",
    "latent_dimension = 200\n",
    "d = 5000\n",
    "m = min([5 * latent_dimension, 5 * d])\n",
    "z = problems.GenerativePhaseRetrievalProblem(\n",
    "    m=m, d=d, latent_dimension=latent_dimension\n",
    ")\n",
    "init = z.initializer(0.5)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=20, eta_est=2)\n",
    "polyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=1, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def polyak_closure():\n",
    "    polyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 50\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=polyak_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=polyak_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Phase retrieval with a generative prior.\\n Signal dimension {}\\n Number of Latent parameters: {}\\n Number of measurements {}\".format(\n",
    "        latent_dimension, d, m\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression with $\\ell_2$ regularization: coupling gradient descent and SuperPolyak on Norm of gradient.\n",
    "\n",
    "In this example, we consider an $\\ell_2$ regularized logistic regression problem on random Gaussian data. At a high-level, the problem is\n",
    "\n",
    "$$\n",
    "\\min  l(x)\n",
    "$$\n",
    "\n",
    "where $l$ is a smooth and strongly convex problem. As our fallback method, we will use the gradient descent algorithm, which converges linearly on this problem.\n",
    "\n",
    "For our loss function $f$, we simply use the norm of the gradient of $l$:\n",
    "$$\n",
    "f(x) = \\|\\nabla l(x)\\|.\n",
    "$$\n",
    "For this problem class, $f(x) = 0$ precisely when $x$ minimizes $l$.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import torch\n",
    "\n",
    "# add time to the imports\n",
    "import time\n",
    "\n",
    "d = 100000\n",
    "m = 1000\n",
    "z = problems.LogisticRegressionProblem(m=m, d=d, l2_penalty=0.0001)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.QR\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(\n",
    "    param, max_elts=2, eta_est=2, linsys_solver=linsys_solver\n",
    ")\n",
    "sgd_optimizer = torch.optim.SGD(param, lr=z.lr)\n",
    "loss_function = z.norm_grad()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sgd_closure():\n",
    "    sgd_optimizer.zero_grad()\n",
    "    loss = z.loss()(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "# start timer\n",
    "start = time.time()\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=sgd_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=sgd_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-16,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "# end timer\n",
    "end = time.time()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Logistic regression problem with {} parameters\".format(d))\n",
    "# modify the title so it also includes the number of samples `m` and the l2 penalty parameter\n",
    "plt.title(\n",
    "    \"Logistic regression problem with {} parameters, {} samples, and l2 penalty {}\".format(\n",
    "        d, m, z.l2_penalty\n",
    "    )\n",
    ")\n",
    "plt.show()\n",
    "print(\"Elapsed time: {} seconds\".format(end - start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import problems\n",
    "import torch\n",
    "from torchmin import Minimizer\n",
    "import time\n",
    "\n",
    "d = 100000\n",
    "m = 1000\n",
    "z = problems.LogisticRegressionProblem(m=m, d=d, l2_penalty=0.0001)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "options = dict(\n",
    "    cg_max_iter=500, xtol=1e-32, disp=1, normp=2, max_iter=1, line_search=\"none\"\n",
    ")\n",
    "Newton_optimizer = Minimizer(param, method=\"Newton-CG\", options=options)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def Newton_closure():\n",
    "    Newton_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    # loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def grad_norm_closure():\n",
    "    Newton_optimizer.zero_grad()\n",
    "    loss = z.norm_grad()(init)\n",
    "    # loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "max_oracle_calls = 10\n",
    "gap = [Newton_closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "# start timer\n",
    "start = time.time()\n",
    "while (\n",
    "    grad_norm_closure().item() > 1e-16\n",
    "    and cumulative_oracle_calls[-1] < max_oracle_calls\n",
    "):\n",
    "    Newton_optimizer.step(Newton_closure)\n",
    "    print(\n",
    "        \"Iteration: \",\n",
    "        cumulative_oracle_calls[-1],\n",
    "        \", Loss: \",\n",
    "        grad_norm_closure().item(),\n",
    "    )\n",
    "    cumulative_oracle_calls.append(2 + cumulative_oracle_calls[-1])\n",
    "    gap.append(grad_norm_closure().item())\n",
    "# end timer\n",
    "end = time.time()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Logistic regression problem with {} parameters, {} samples, and l2 penalty {}\".format(\n",
    "        d, m, z.l2_penalty\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Elapsed time: {} seconds\".format(end - start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Newton's method on $\\ell_2$ regularized logistic regression problem\n",
    "\n",
    "Here we demonstrate a Newton algorithm on the same problem as in the previous cell. In our implementation, we use a CG type algorithm to solve the linear system."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import problems\n",
    "import torch\n",
    "import time\n",
    "import newtoncg\n",
    "\n",
    "d = 1000\n",
    "m = 1000\n",
    "z = problems.LogisticRegressionProblem(m=m, d=d, l2_penalty=0.0001)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "Newton_optimizer = newtoncg.NewtonCG(param)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def Newton_closure(x):\n",
    "    Newton_optimizer.zero_grad()\n",
    "    loss = loss_function(x)\n",
    "    # loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def grad_norm_closure():\n",
    "    Newton_optimizer.zero_grad()\n",
    "    loss = z.norm_grad()(init)\n",
    "    # loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "max_oracle_calls = 100\n",
    "gap = [Newton_closure(init).item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "# start timer\n",
    "start = time.time()\n",
    "while (\n",
    "    grad_norm_closure().item() > 1e-16\n",
    "    and cumulative_oracle_calls[-1] < max_oracle_calls\n",
    "):\n",
    "    Newton_optimizer.step(Newton_closure)\n",
    "    print(\n",
    "        \"Iteration: \",\n",
    "        cumulative_oracle_calls[-1],\n",
    "        \", Loss: \",\n",
    "        grad_norm_closure().item(),\n",
    "    )\n",
    "    cumulative_oracle_calls.append(1 + cumulative_oracle_calls[-1])\n",
    "    gap.append(grad_norm_closure().item())\n",
    "# end timer\n",
    "end = time.time()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Logistic regression problem with {} parameters, {} samples, and l2 penalty {}\".format(\n",
    "        d, m, z.l2_penalty\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Elapsed time: {} seconds\".format(end - start))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}