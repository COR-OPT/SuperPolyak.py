{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Optimizing the l1 norm with superpolyak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# A pytorch compatible version of the above code.\n",
    "import AlternatingProjections\n",
    "import util\n",
    "import numpy as np\n",
    "import torch\n",
    "import SuperPolyak\n",
    "\n",
    "d = 5\n",
    "max_elts = d\n",
    "x = torch.randn(d, requires_grad=True, dtype=torch.double)\n",
    "\n",
    "\n",
    "def f():\n",
    "    return torch.sum(abs(x))\n",
    "\n",
    "\n",
    "optimizer = SuperPolyak.SuperPolyak([x], max_elts=max_elts, eta_est=1.5)\n",
    "# Closure function to allow us to call backward.\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = f()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "current_iter = 0\n",
    "while closure().item() > 1e-20 and current_iter < 100:\n",
    "    loss, bundle_index = optimizer.step(closure)\n",
    "    print(\"f(y)\", closure().item())\n",
    "    print(\"Bundle index\", bundle_index)\n",
    "    current_iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Optimizing a small neural network with SuperPolyak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fitting a small neural network with pytorch.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import SuperPolyak\n",
    "\n",
    "\n",
    "input_size = 100\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2000, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(2000, 1, dtype=torch.double)\n",
    "        # add a convolutional layer of the appropriate __sizeof__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # add another relu layer\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# Let d be the number of parameters in net\n",
    "d = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"number of parameters\", d)\n",
    "max_elts = int(200)\n",
    "print(\"bundle size\", max_elts)\n",
    "\n",
    "# fake training data\n",
    "x = torch.randn(1000, input_size, dtype=torch.double)\n",
    "y = net(x).detach().clone().requires_grad_(False)\n",
    "# Reset the parameters\n",
    "net = Net()\n",
    "\n",
    "# a loss function\n",
    "def loss_function():\n",
    "    return sum(torch.abs(net(x) - y))\n",
    "\n",
    "\n",
    "# a closure function to allow us to call backward\n",
    "max_oracle_calls = 10000\n",
    "params = list(net.parameters())\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.LSMR\n",
    "optimizer = SuperPolyak.SuperPolyak(\n",
    "    params, max_elts=max_elts, eta_est=0.1, linsys_solver=linsys_solver\n",
    ")\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\n",
    "        \"Iteration: \",\n",
    "        cumulative_oracle_calls[-1],\n",
    "        \", Loss: \",\n",
    "        closure().item(),\n",
    "        \", Bundle_exit_step \",\n",
    "        bundle_idx,\n",
    "    )\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# An illustration of the problems.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import problems\n",
    "\n",
    "d = 100\n",
    "k = 2\n",
    "m = 4 * d * k\n",
    "z = problems.MaxAffineRegressionProblem(m=m, d=100, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "optimizer = SuperPolyak.SuperPolyak(param, max_elts=40, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "# A short loop that applies the SuperPolyak subgradient method to the loss function\n",
    "max_oracle_calls = 1000\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\n",
    "        \"Iteration: {}, Loss: {}, Bundle idx: {}\".format(\n",
    "            cumulative_oracle_calls[-1], loss, bundle_idx\n",
    "        )\n",
    "    )\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# An illustration of coupling SuperPolyak with the Polyak subgradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "\n",
    "d = 100\n",
    "k = 2\n",
    "m = 4 * d * k\n",
    "z = problems.MaxAffineRegressionProblem(m=m, d=100, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=40, eta_est=2)\n",
    "polyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=1, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "max_inner_iter = 10\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=closure,\n",
    "    fallback_closure=closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=polyak_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Max affine regression with {} parameters\".format(d * 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Illustration of using the prox-gradient method as a fallback method for Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import proxgradient\n",
    "import torch\n",
    "\n",
    "d = 1000\n",
    "k = 2\n",
    "m = 4 * k\n",
    "z = problems.LassoProblem(m=m, d=d, k=k)\n",
    "init = z.initializer(0.00001)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=40, eta_est=2)\n",
    "proxgradient_optimizer = proxgradient.ProxGradient(\n",
    "    param, proxs=[z.prox], lr=z.prox_step\n",
    ")\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def fallback_closure():\n",
    "    proxgradient_optimizer.zero_grad()\n",
    "    loss = 0.5 * torch.sum(torch.square(z.A @ init - z.y))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=fallback_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=proxgradient_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Lasso problem with {} parameters\".format(d * 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Phase retrieval on distance functions with Alternating Projections Fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import AlternatingProjections\n",
    "import torch\n",
    "\n",
    "d = 100\n",
    "m = 8 * d\n",
    "z = problems.PhaseRetrievalProblem(m=m, d=d)\n",
    "init = z.initializer_altproj(0.1)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=10, eta_est=0.5)\n",
    "projs = [z.alternating_projections_step()]\n",
    "AlternatingProjections_optimizer = AlternatingProjections.AlternatingProjections(\n",
    "    param, projs=projs\n",
    ")\n",
    "loss_function = z.loss_altproj()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=superpolyak_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=AlternatingProjections_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Complex phase retrieval problem problem with {} parameters and {} measurements.\".format(\n",
    "        d * 2, m\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logistic regression with l2 regularization: coupling gradient descent and SuperPolyak on Norm of gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import torch\n",
    "\n",
    "d = 1000\n",
    "m = 10000\n",
    "z = problems.LogisticRegressionProblem(m=m, d=d, l2_penalty=0.0001)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=10, eta_est=2)\n",
    "sgd_optimizer = torch.optim.SGD(param, lr=z.lr)\n",
    "loss_function = z.norm_grad()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sgd_closure():\n",
    "    sgd_optimizer.zero_grad()\n",
    "    loss = z.loss()(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=sgd_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=sgd_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Logistic regression problem with {} parameters\".format(d))\n",
    "# modify the title so it also includes the number of samples `m` and the l2 penalty parameter\n",
    "plt.title(\n",
    "    \"Logistic regression problem with {} parameters, {} samples, and l2 penalty {}\".format(\n",
    "        d, m, z.l2_penalty\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase retrieval with a generative prior"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import torch\n",
    "\n",
    "latent_dimension = 200\n",
    "d = 5000\n",
    "m = min([5 * latent_dimension, 5 * d])\n",
    "z = problems.GenerativePhaseRetrievalProblem(\n",
    "    m=m, d=d, latent_dimension=latent_dimension\n",
    ")\n",
    "init = z.initializer(0.5)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=20, eta_est=2)\n",
    "polyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts=1, eta_est=2)\n",
    "# sgd_optimizer = torch.optim.SGD(param, lr=1)\n",
    "loss_function = z.loss()\n",
    "\n",
    "\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def polyak_closure():\n",
    "    polyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 50\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(\n",
    "    superpolyak_closure=superpolyak_closure,\n",
    "    fallback_closure=polyak_closure,\n",
    "    superpolyak_optimizer=superpolyak_optimizer,\n",
    "    fallback_optimizer=polyak_optimizer,\n",
    "    max_inner_iter=max_inner_iter,\n",
    "    tol=1e-10,\n",
    "    max_outer_iter=max_outer_iter,\n",
    "    verbose=True,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Phase retrieval with a generative prior.\\n Signal dimension {}\\n Number of Latent parameters: {}\\n Number of measurements {}\".format(\n",
    "        latent_dimension, d, m\n",
    "    )\n",
    ")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4723082860782584\n",
      "Iteration:  0 , Loss:  0.13742093894838775\n",
      "Iteration:  1 , Loss:  0.1374209389483877\n",
      "Iteration:  2 , Loss:  0.1374209389483877\n",
      "Iteration:  3 , Loss:  0.13742093894838767\n",
      "Iteration:  4 , Loss:  0.13742093894838767\n",
      "Iteration:  5 , Loss:  0.13742093894838767\n",
      "Iteration:  6 , Loss:  0.13742093894838767\n",
      "Iteration:  7 , Loss:  0.13742093894838767\n",
      "Iteration:  8 , Loss:  0.13742093894838767\n",
      "Iteration:  9 , Loss:  0.13742093894838767\n",
      "Iteration:  10 , Loss:  0.13742093894838767\n",
      "Iteration:  11 , Loss:  0.13742093894838767\n",
      "Iteration:  12 , Loss:  0.13742093894838767\n",
      "Iteration:  13 , Loss:  0.13742093894838767\n",
      "Iteration:  14 , Loss:  0.13742093894838767\n",
      "Iteration:  15 , Loss:  0.13742093894838767\n",
      "Iteration:  16 , Loss:  0.13742093894838767\n",
      "Iteration:  17 , Loss:  0.13742093894838767\n",
      "Iteration:  18 , Loss:  0.13742093894838767\n",
      "Iteration:  19 , Loss:  0.13742093894838767\n",
      "Iteration:  20 , Loss:  0.13742093894838767\n",
      "Iteration:  21 , Loss:  0.13742093894838767\n",
      "Iteration:  22 , Loss:  0.13742093894838767\n",
      "Iteration:  23 , Loss:  0.13742093894838767\n",
      "Iteration:  24 , Loss:  0.13742093894838767\n",
      "Iteration:  25 , Loss:  0.13742093894838767\n",
      "Iteration:  26 , Loss:  0.13742093894838767\n",
      "Iteration:  27 , Loss:  0.13742093894838767\n",
      "Iteration:  28 , Loss:  0.13742093894838767\n",
      "Iteration:  29 , Loss:  0.13742093894838767\n",
      "Iteration:  30 , Loss:  0.13742093894838767\n",
      "Iteration:  31 , Loss:  0.13742093894838767\n",
      "Iteration:  32 , Loss:  0.13742093894838767\n",
      "Iteration:  33 , Loss:  0.13742093894838767\n",
      "Iteration:  34 , Loss:  0.13742093894838767\n",
      "Iteration:  35 , Loss:  0.13742093894838767\n",
      "Iteration:  36 , Loss:  0.13742093894838767\n",
      "Iteration:  37 , Loss:  0.13742093894838767\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [6]\u001B[0m, in \u001B[0;36m<cell line: 27>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mprint\u001B[39m(gap[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m Newton_closure()\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1e-10\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m cumulative_oracle_calls[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m<\u001B[39m max_oracle_calls:\n\u001B[0;32m---> 28\u001B[0m     \u001B[43mNewton_optimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNewton_closure\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\n\u001B[1;32m     30\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIteration: \u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     31\u001B[0m         cumulative_oracle_calls[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m],\n\u001B[1;32m     32\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     33\u001B[0m         Newton_closure()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     34\u001B[0m     )\n\u001B[1;32m     35\u001B[0m     cumulative_oracle_calls\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m cumulative_oracle_calls[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/optim/optimizer.py:88\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     86\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[0;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torchmin/optim/minimizer.py:194\u001B[0m, in \u001B[0;36mMinimizer.step\u001B[0;34m(self, closure)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;66;03m# perform parameter update\u001B[39;00m\n\u001B[1;32m    193\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m {k:v \u001B[38;5;28;01mfor\u001B[39;00m k,v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_groups[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mparams\u001B[39m\u001B[38;5;124m'\u001B[39m}\n\u001B[0;32m--> 194\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result \u001B[38;5;241m=\u001B[39m \u001B[43mminimize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;66;03m# set final value\u001B[39;00m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_flat_param(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result\u001B[38;5;241m.\u001B[39mx)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torchmin/minimize.py:97\u001B[0m, in \u001B[0;36mminimize\u001B[0;34m(fun, x0, method, max_iter, tol, options, callback, disp, return_all)\u001B[0m\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _minimize_newton_cg(fun, x0, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnewton-exact\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m---> 97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_minimize_newton_exact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdogleg\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m     99\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _minimize_dogleg(fun, x0, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[0;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torchmin/newton.py:295\u001B[0m, in \u001B[0;36m_minimize_newton_exact\u001B[0;34m(fun, x0, lr, max_iter, line_search, xtol, normp, tikhonov, handle_npd, callback, disp, return_all)\u001B[0m\n\u001B[1;32m    293\u001B[0m \u001B[38;5;66;03m# initial settings\u001B[39;00m\n\u001B[1;32m    294\u001B[0m x \u001B[38;5;241m=\u001B[39m x0\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mclone(memory_format\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mcontiguous_format)\n\u001B[0;32m--> 295\u001B[0m f, g, _, hess \u001B[38;5;241m=\u001B[39m \u001B[43mclosure\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    296\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tikhonov \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    297\u001B[0m     hess\u001B[38;5;241m.\u001B[39mdiagonal()\u001B[38;5;241m.\u001B[39madd_(tikhonov)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torchmin/optim/minimizer.py:139\u001B[0m, in \u001B[0;36mMinimizer.closure\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    137\u001B[0m         hess \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(numel, numel, dtype\u001B[38;5;241m=\u001B[39mgrad\u001B[38;5;241m.\u001B[39mdtype, device\u001B[38;5;241m=\u001B[39mgrad\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    138\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(numel):\n\u001B[0;32m--> 139\u001B[0m             hess[i] \u001B[38;5;241m=\u001B[39m \u001B[43mhvp\u001B[49m\u001B[43m(\u001B[49m\u001B[43meye\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sf_value(f\u001B[38;5;241m=\u001B[39mf\u001B[38;5;241m.\u001B[39mdetach(), grad\u001B[38;5;241m=\u001B[39mgrad_out\u001B[38;5;241m.\u001B[39mdetach(), hessp\u001B[38;5;241m=\u001B[39mhessp, hess\u001B[38;5;241m=\u001B[39mhess)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torchmin/optim/minimizer.py:126\u001B[0m, in \u001B[0;36mMinimizer.closure.<locals>.hvp\u001B[0;34m(v)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mhvp\u001B[39m(v):\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m v\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m==\u001B[39m grad\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m--> 126\u001B[0m     \u001B[43mgrad\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgradient\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gather_flat_grad()\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;241m-\u001B[39m grad_accum\n\u001B[1;32m    128\u001B[0m     grad_accum\u001B[38;5;241m.\u001B[39madd_(output)\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/_tensor.py:400\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    392\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    393\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    394\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    398\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    399\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 400\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import problems\n",
    "import torch\n",
    "from torchmin import Minimizer\n",
    "\n",
    "d = 1000\n",
    "m = 10\n",
    "z = problems.LogisticRegressionProblem(m=m, d=d, l2_penalty=0.001)\n",
    "init = z.initializer(.001)\n",
    "param = [init]\n",
    "Newton_optimizer = Minimizer(param, method=\"Newton-exact\")\n",
    "loss_function = z.loss()\n",
    "\n",
    "def Newton_closure():\n",
    "    Newton_optimizer.zero_grad()\n",
    "    loss = loss_function(param[0])\n",
    "    # loss.backward()\n",
    "    return loss\n",
    "\n",
    "max_oracle_calls = 100\n",
    "gap = [Newton_closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "print(gap[-1])\n",
    "\n",
    "while Newton_closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    Newton_optimizer.step(Newton_closure)\n",
    "    print(\n",
    "        \"Iteration: \",\n",
    "        cumulative_oracle_calls[-1],\n",
    "        \", Loss: \",\n",
    "        Newton_closure().item()\n",
    "    )\n",
    "    cumulative_oracle_calls.append(1 + cumulative_oracle_calls[-1])\n",
    "    gap.append(Newton_closure().item())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\n",
    "    \"Logistic regression problem with {} parameters, {} samples, and l2 penalty {}\".format(\n",
    "        d, m, z.l2_penalty\n",
    "    )\n",
    ")\n",
    "plt.show()\n",
    "print(Newton_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}