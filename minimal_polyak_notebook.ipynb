{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing the l1 norm with superpolyak\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A pytorch compatible version of the above code.\n",
    "import AlternatingProjections\n",
    "import util\n",
    "import numpy as np\n",
    "import torch\n",
    "import SuperPolyak\n",
    "\n",
    "d = 5\n",
    "max_elts = d\n",
    "x = torch.randn(d, requires_grad=True, dtype=torch.double)\n",
    "\n",
    "def f():\n",
    "    return torch.sum(abs(x))\n",
    "\n",
    "optimizer=SuperPolyak.SuperPolyak([x], max_elts=max_elts, eta_est=1.5)\n",
    "# Closure function to allow us to call backward.\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = f()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "current_iter = 0\n",
    "while closure().item() > 1e-20 and current_iter < 100:\n",
    "    loss, bundle_index = optimizer.step(closure)\n",
    "    print(\"f(y)\", closure().item())\n",
    "    print(\"Bundle index\", bundle_index)\n",
    "    current_iter += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing a small neural network with SuperPolyak"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitting a small neural network with pytorch.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import SuperPolyak\n",
    "\n",
    "\n",
    "input_size = 100\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2000, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(2000, 1, dtype=torch.double)\n",
    "        # add a convolutional layer of the appropriate __sizeof__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # add another relu layer\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "# Let d be the number of parameters in net\n",
    "d = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"number of parameters\", d)\n",
    "max_elts = int(200)\n",
    "print(\"bundle size\", max_elts)\n",
    "\n",
    "# fake training data\n",
    "x = torch.randn(1000, input_size, dtype=torch.double)\n",
    "y = net(x).detach().clone().requires_grad_(False)\n",
    "# Reset the parameters\n",
    "net = Net()\n",
    "\n",
    "# a loss function\n",
    "def loss_function():\n",
    "    return sum(torch.abs(net(x) - y))\n",
    "\n",
    "# a closure function to allow us to call backward\n",
    "max_oracle_calls = 10000\n",
    "params = list(net.parameters())\n",
    "linsys_solver=SuperPolyak.BundleLinearSystemSolver.LSMR\n",
    "optimizer = SuperPolyak.SuperPolyak(params, max_elts=max_elts, eta_est=.1, linsys_solver=linsys_solver)\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\"Iteration: \", cumulative_oracle_calls[-1], \", Loss: \", closure().item(), \", Bundle_exit_step \", bundle_idx)\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An illustration of the problems.py module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import problems\n",
    "d = 100\n",
    "k=2\n",
    "m = 4*d*k\n",
    "z = problems.MaxAffineRegressionProblem(m=m,d=100, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "optimizer = SuperPolyak.SuperPolyak(param, max_elts = 40, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "# A short loop that applies the SuperPolyak subgradient method to the loss function\n",
    "max_oracle_calls = 1000\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\"Iteration: {}, Loss: {}, Bundle idx: {}\".format(cumulative_oracle_calls[-1], loss, bundle_idx))\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An illustration of coupling SuperPolyak with the Polyak subgradient method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "d = 100\n",
    "k=2\n",
    "m = 4*d*k\n",
    "z = problems.MaxAffineRegressionProblem(m=m,d=100, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 40, eta_est=2)\n",
    "polyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 1, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "def closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "max_inner_iter = 10\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(closure_superpolyak=closure, closure_fallback=closure, superpolyak_optimizer=superpolyak_optimizer, fallback_optimizer=polyak_optimizer, max_inner_iter=max_inner_iter, tol = 1e-10, max_outer_iter=max_outer_iter, verbose=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Max affine regression with {} parameters\".format(d*2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Illustration of using the prox-gradient method as a fallback method for Lasso"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LassoProblem.__init__() got an unexpected keyword argument 'l2_penalty'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [1]\u001B[0m, in \u001B[0;36m<cell line: 11>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m4\u001B[39m\u001B[38;5;241m*\u001B[39mk\n\u001B[1;32m     10\u001B[0m l2_penalty \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1e-2\u001B[39m\n\u001B[0;32m---> 11\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[43mproblems\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLassoProblem\u001B[49m\u001B[43m(\u001B[49m\u001B[43mm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mm\u001B[49m\u001B[43m,\u001B[49m\u001B[43md\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43md\u001B[49m\u001B[43m,\u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml2_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ml2_penalty\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     12\u001B[0m init \u001B[38;5;241m=\u001B[39m z\u001B[38;5;241m.\u001B[39minitializer(\u001B[38;5;241m.001\u001B[39m)\n\u001B[1;32m     13\u001B[0m param \u001B[38;5;241m=\u001B[39m [init]\n",
      "\u001B[0;31mTypeError\u001B[0m: LassoProblem.__init__() got an unexpected keyword argument 'l2_penalty'"
     ]
    }
   ],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import proxgradient\n",
    "import torch\n",
    "d = 1000\n",
    "k=2\n",
    "m = 4*k\n",
    "z = problems.LassoProblem(m=m,d=d,k=k)\n",
    "init = z.initializer(.00001)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 40, eta_est=2)\n",
    "proxgradient_optimizer = proxgradient.ProxGradient(param, proxs=[z.prox], lr=z.prox_step)\n",
    "loss_function = z.loss()\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "def fallback_closure():\n",
    "    proxgradient_optimizer.zero_grad()\n",
    "    loss = .5*torch.sum(torch.square(z.A @ init - z.y))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list \\\n",
    "    = util.superpolyak_coupled_with_fallback(superpolyak_closure=superpolyak_closure,\n",
    "                                             fallback_closure=fallback_closure,\n",
    "                                             superpolyak_optimizer=superpolyak_optimizer,\n",
    "                                             fallback_optimizer=proxgradient_optimizer,\n",
    "                                             max_inner_iter=max_inner_iter,\n",
    "                                             tol = 1e-10,\n",
    "                                             max_outer_iter=max_outer_iter,\n",
    "                                             verbose=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Lasso problem with {} parameters\".format(d*2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phase retrieval on distance functions with Alternating Projections Fallback\n",
    "This is broken, please take a look vas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import AlternatingProjections\n",
    "import torch\n",
    "d = 100\n",
    "m = 4*d\n",
    "z = problems.PhaseRetrievalProblem(m=m,d=d)\n",
    "init = z.initializer_altproj(0.0)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 10, eta_est=2)\n",
    "projs = [z.alternating_projections_step()]\n",
    "proxgradient_optimizer = AlternatingProjections.AlternatingProjecions(param, projs=projs)\n",
    "loss_function = z.loss_altproj()\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list \\\n",
    "    = util.superpolyak_coupled_with_fallback(superpolyak_closure=superpolyak_closure,\n",
    "                                             fallback_closure=superpolyak_closure,\n",
    "                                             superpolyak_optimizer=superpolyak_optimizer,\n",
    "                                             fallback_optimizer=proxgradient_optimizer,\n",
    "                                             max_inner_iter=max_inner_iter,\n",
    "                                             tol = 1e-10,\n",
    "                                             max_outer_iter=max_outer_iter,\n",
    "                                             verbose=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Lasso problem with {} parameters\".format(d*2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression with l2 regularization: coupling gradient descent and SuperPolyak on Norm of gradient."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import torch\n",
    "d = 1000\n",
    "m = 20\n",
    "z = problems.LogisticRegressionProblem(m=m,d=d,l2_penalty=0.0001)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "linsys_solver = SuperPolyak.BundleLinearSystemSolver.LSMR\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 10, eta_est=2, linsys_solver=linsys_solver)\n",
    "sgd_optimizer = torch.optim.SGD(param, lr=z.lr)\n",
    "loss_function = z.norm_grad()\n",
    "def superpolyak_closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "def sgd_closure():\n",
    "    sgd_optimizer.zero_grad()\n",
    "    loss = z.loss()(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "print(superpolyak_closure().item())\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list \\\n",
    "    = util.superpolyak_coupled_with_fallback(superpolyak_closure=superpolyak_closure,\n",
    "                                             fallback_closure=sgd_closure,\n",
    "                                             superpolyak_optimizer=superpolyak_optimizer,\n",
    "                                             fallback_optimizer=sgd_optimizer,\n",
    "                                             max_inner_iter=max_inner_iter,\n",
    "                                             tol = 1e-10,\n",
    "                                             max_outer_iter=max_outer_iter,\n",
    "                                             verbose=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Logistic regression problem with {} parameters\".format(d))\n",
    "# modify the title so it also includes the number of samples `m` and the l2 penalty parameter\n",
    "plt.title(\"Logistic regression problem with {} parameters, {} samples, and l2 penalty {}\".format(d,m,z.l2_penalty))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}