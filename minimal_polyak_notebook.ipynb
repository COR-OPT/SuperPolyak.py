{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing the l1 norm with superpolyak\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A pytorch compatible version of the above code.\n",
    "import util\n",
    "import numpy as np\n",
    "import torch\n",
    "import SuperPolyak\n",
    "\n",
    "d = 5\n",
    "max_elts = d\n",
    "x = torch.randn(d, requires_grad=True, dtype=torch.double)\n",
    "\n",
    "def f():\n",
    "    return torch.sum(abs(x))\n",
    "\n",
    "optimizer=SuperPolyak.SuperPolyak([x], max_elts=max_elts, eta_est=1.5)\n",
    "# Closure function to allow us to call backward.\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = f()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "current_iter = 0\n",
    "while closure().item() > 1e-20 and current_iter < 100:\n",
    "    loss, bundle_index = optimizer.step(closure)\n",
    "    print(\"f(y)\", closure().item())\n",
    "    print(\"Bundle index\", bundle_index)\n",
    "    current_iter += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing a small neural network with SuperPolyak"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitting a small neural network with pytorch.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import SuperPolyak\n",
    "\n",
    "\n",
    "input_size = 100\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2000, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(2000, 1, dtype=torch.double)\n",
    "        # add a convolutional layer of the appropriate __sizeof__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # add another relu layer\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "# Let d be the number of parameters in net\n",
    "d = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"number of parameters\", d)\n",
    "max_elts = int(200)\n",
    "print(\"bundle size\", max_elts)\n",
    "\n",
    "# fake training data\n",
    "x = torch.randn(1000, input_size, dtype=torch.double)\n",
    "y = net(x).detach().clone().requires_grad_(False)\n",
    "# Reset the parameters\n",
    "net = Net()\n",
    "\n",
    "# a loss function\n",
    "def loss_function():\n",
    "    return sum(torch.abs(net(x) - y))\n",
    "\n",
    "# a closure function to allow us to call backward\n",
    "max_oracle_calls = 10000\n",
    "params = list(net.parameters())\n",
    "linsys_solver=SuperPolyak.BundleLinearSystemSolver.LSMR\n",
    "optimizer = SuperPolyak.SuperPolyak(params, max_elts=max_elts, eta_est=.1, linsys_solver=linsys_solver)\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\"Iteration: \", cumulative_oracle_calls[-1], \", Loss: \", closure().item(), \", Bundle_exit_step \", bundle_idx)\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An illustration of the problems.py module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import problems\n",
    "d = 100\n",
    "k=2\n",
    "m = 4*d*k\n",
    "z = problems.MaxAffineRegressionProblem(m=m,d=100, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "optimizer = SuperPolyak.SuperPolyak(param, max_elts = 40, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "# A short loop that applies the SuperPolyak subgradient method to the loss function\n",
    "max_oracle_calls = 1000\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\"Iteration: {}, Loss: {}, Bundle idx: {}\".format(cumulative_oracle_calls[-1], loss, bundle_idx))\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An illustration of coupling SuperPolyak with the Polyak subgradient method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "d = 100\n",
    "k=2\n",
    "m = 4*d*k\n",
    "z = problems.MaxAffineRegressionProblem(m=m,d=100, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 40, eta_est=2)\n",
    "polyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 1, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "def closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "max_inner_iter = 10\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(closure_superpolyak=closure, closure_fallback=closure, superpolyak_optimizer=superpolyak_optimizer, fallback_optimizer=polyak_optimizer, max_inner_iter=max_inner_iter, tol = 1e-10, max_outer_iter=max_outer_iter, verbose=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Max affine regression with {} parameters\".format(d*2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Illustration of using the prox-gradient method as a fallback method for Lasso"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback step! Current oracle evaluation:  50 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  100 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  150 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  200 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  250 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  300 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  350 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  400 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  450 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  500 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  550 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  600 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  650 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  700 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  750 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  800 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  850 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  900 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  950 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1000 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1050 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1100 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1150 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1200 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1250 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1300 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1350 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1400 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1450 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1500 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1550 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1600 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1650 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1700 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1750 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1800 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1850 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1900 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  1950 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2000 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2050 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2100 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2150 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2200 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2250 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2300 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2350 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2400 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2450 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2500 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2550 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2600 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2650 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2700 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2750 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2800 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2850 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2900 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  2950 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3000 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3050 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3100 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3150 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3200 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3250 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3300 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3350 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3400 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3450 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3500 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3550 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3600 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3650 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3700 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3750 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3800 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3850 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3900 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  3950 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4000 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4050 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4100 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4150 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4200 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4250 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4300 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4350 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4400 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4450 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4500 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4550 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4600 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4650 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4700 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4750 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4800 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4850 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4900 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  4950 ,Loss =  0.017956725353760146\n",
      "Fallback step! Current oracle evaluation:  5000 ,Loss =  0.017956725353760146\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import proxgradient\n",
    "d = 100\n",
    "k=2\n",
    "m = 4*k\n",
    "z = problems.LassoProblem(m=m,d=d, k=k)\n",
    "init = z.initializer(.01)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 40, eta_est=2)\n",
    "proxgradient_optimizer = proxgradient.ProxGradient(param, proxs=[z.prox], lr=z.Ï„/10)\n",
    "loss_function = z.loss()\n",
    "def closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "def closure_fallback():\n",
    "    proxgradient_optimizer.zero_grad()\n",
    "    loss = .5*torch.linalg.sum(torch.square(z.A @ init - z.y))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "max_inner_iter = 10\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(closure_superpolyak=closure, closure_fallback=closure, superpolyak_optimizer=superpolyak_optimizer, fallback_optimizer=proxgradient_optimizer, max_inner_iter=max_inner_iter, tol = 1e-10, max_outer_iter=max_outer_iter, verbose=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Lasso problem with {} parameters\".format(d*2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}