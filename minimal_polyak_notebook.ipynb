{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing the l1 norm with superpolyak\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# A pytorch compatible version of the above code.\n",
    "import util\n",
    "import numpy as np\n",
    "import torch\n",
    "import SuperPolyak\n",
    "\n",
    "d = 5\n",
    "max_elts = d\n",
    "x = torch.randn(d, requires_grad=True, dtype=torch.double)\n",
    "\n",
    "def f():\n",
    "    return torch.sum(abs(x))\n",
    "\n",
    "optimizer=SuperPolyak.SuperPolyak([x], max_elts=max_elts, eta_est=1.5)\n",
    "# Closure function to allow us to call backward.\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = f()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "current_iter = 0\n",
    "while closure().item() > 1e-20 and current_iter < 100:\n",
    "    loss, bundle_index = optimizer.step(closure)\n",
    "    print(\"f(y)\", closure().item())\n",
    "    print(\"Bundle index\", bundle_index)\n",
    "    current_iter += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizing a small neural network with SuperPolyak"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fitting a small neural network with pytorch.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import SuperPolyak\n",
    "\n",
    "\n",
    "input_size = 100\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2000, dtype=torch.double)\n",
    "        self.fc2 = nn.Linear(2000, 1, dtype=torch.double)\n",
    "        # add a convolutional layer of the appropriate __sizeof__\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # add another relu layer\n",
    "        # x = F.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "# Let d be the number of parameters in net\n",
    "d = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(\"number of parameters\", d)\n",
    "max_elts = int(200)\n",
    "print(\"bundle size\", max_elts)\n",
    "\n",
    "# fake training data\n",
    "x = torch.randn(1000, input_size, dtype=torch.double)\n",
    "y = net(x).detach().clone().requires_grad_(False)\n",
    "# Reset the parameters\n",
    "net = Net()\n",
    "\n",
    "# a loss function\n",
    "def loss_function():\n",
    "    return sum(torch.abs(net(x) - y))\n",
    "\n",
    "# a closure function to allow us to call backward\n",
    "max_oracle_calls = 10000\n",
    "params = list(net.parameters())\n",
    "linsys_solver=SuperPolyak.BundleLinearSystemSolver.LSMR\n",
    "optimizer = SuperPolyak.SuperPolyak(params, max_elts=max_elts, eta_est=.1, linsys_solver=linsys_solver)\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\"Iteration: \", cumulative_oracle_calls[-1], \", Loss: \", closure().item(), \", Bundle_exit_step \", bundle_idx)\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An illustration of the problems.py module"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import SuperPolyak\n",
    "import problems\n",
    "d = 100\n",
    "k=2\n",
    "m = 4*d*k\n",
    "z = problems.MaxAffineRegressionProblem(m=m,d=100, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "optimizer = SuperPolyak.SuperPolyak(param, max_elts = 40, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "# A short loop that applies the SuperPolyak subgradient method to the loss function\n",
    "max_oracle_calls = 1000\n",
    "while closure().item() > 1e-10 and cumulative_oracle_calls[-1] < max_oracle_calls:\n",
    "    loss, bundle_idx = optimizer.step(closure)\n",
    "    print(\"Iteration: {}, Loss: {}, Bundle idx: {}\".format(cumulative_oracle_calls[-1], loss, bundle_idx))\n",
    "    cumulative_oracle_calls.append(bundle_idx + cumulative_oracle_calls[-1])\n",
    "    gap.append(closure().item())\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(cumulative_oracle_calls, gap)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"2 layer neural network with {} parameters\".format(d))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# An illustration of coupling SuperPolyak with the Polyak subgradient method"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "d = 100\n",
    "k=2\n",
    "m = 4*d*k\n",
    "z = problems.MaxAffineRegressionProblem(m=m,d=100, k=k)\n",
    "init = z.initializer(1)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 40, eta_est=2)\n",
    "polyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 1, eta_est=2)\n",
    "loss_function = z.loss()\n",
    "def closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "max_inner_iter = 10\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(closure_superpolyak=closure, closure_fallback=closure, superpolyak_optimizer=superpolyak_optimizer, fallback_optimizer=polyak_optimizer, max_inner_iter=max_inner_iter, tol = 1e-10, max_outer_iter=max_outer_iter, verbose=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Max affine regression with {} parameters\".format(d*2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Illustration of using the prox-gradient method as a fallback method for Lasso"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import proxgradient\n",
    "d = 100\n",
    "k=2\n",
    "m = 4*k\n",
    "z = problems.LassoProblem(m=m,d=d, k=k)\n",
    "init = z.initializer(.01)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 1, eta_est=2)\n",
    "proxgradient_optimizer = proxgradient.ProxGradient(param, proxs=[z.prox], lr=z.prox_step)\n",
    "loss_function = z.loss()\n",
    "def closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "def closure_fallback():\n",
    "    proxgradient_optimizer.zero_grad()\n",
    "    loss = .5*torch.linalg.sum(torch.square(z.A @ init - z.y))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "max_inner_iter = 100\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(closure_superpolyak=closure, closure_fallback=closure, superpolyak_optimizer=superpolyak_optimizer, fallback_optimizer=proxgradient_optimizer, max_inner_iter=max_inner_iter, tol = 1e-10, max_outer_iter=max_outer_iter, verbose=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Lasso problem with {} parameters\".format(d*2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Illustration of using the prox-gradient method as a fallback method for Lasso"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback step! Current oracle evaluation:  50 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  100 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  150 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  200 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  250 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  300 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  350 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  400 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  450 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  500 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  550 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  600 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  650 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  700 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  750 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  800 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  850 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  900 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  950 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1000 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1050 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1100 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1150 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1200 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1250 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1300 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1350 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1400 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1450 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1500 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1550 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1600 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1650 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1700 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1750 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1800 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1850 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1900 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  1950 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2000 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2050 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2100 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2150 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2200 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2250 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2300 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2350 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2400 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2450 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2500 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2550 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2600 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2650 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2700 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2750 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2800 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2850 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2900 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  2950 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3000 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3050 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3100 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3150 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3200 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3250 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3300 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3350 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3400 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3450 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3500 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3550 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3600 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3650 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3700 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3750 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3800 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3850 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3900 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  3950 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4000 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4050 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4100 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4150 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4200 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4250 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4300 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4350 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4400 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4450 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4500 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4550 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4600 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4650 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4700 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4750 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4800 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4850 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4900 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  4950 ,Loss =  0.01075642539586557\n",
      "Fallback step! Current oracle evaluation:  5000 ,Loss =  0.01075642539586557\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Illustration of util.superpolyak_coupled_with_fallback\n",
    "import SuperPolyak\n",
    "import util\n",
    "import problems\n",
    "import proxgradient\n",
    "d = 100\n",
    "k=2\n",
    "m = 4*k\n",
    "z = problems.LassoProblem(m=m,d=d, k=k)\n",
    "init = z.initializer(.01)\n",
    "param = [init]\n",
    "superpolyak_optimizer = SuperPolyak.SuperPolyak(param, max_elts = 40, eta_est=2)\n",
    "proxgradient_optimizer = proxgradient.ProxGradient(param, proxs=[z.prox], lr=z.τ/10)\n",
    "loss_function = z.loss()\n",
    "def closure():\n",
    "    superpolyak_optimizer.zero_grad()\n",
    "    loss = loss_function(init)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "def closure_fallback():\n",
    "    proxgradient_optimizer.zero_grad()\n",
    "    loss = .5*torch.linalg.sum(torch.square(z.A @ init - z.y))\n",
    "    loss.backward()\n",
    "    return loss\n",
    "gap = [closure().item()]\n",
    "cumulative_oracle_calls = [0]\n",
    "max_inner_iter = 10\n",
    "max_outer_iter = 100\n",
    "oracle_calls, loss_list = util.superpolyak_coupled_with_fallback(closure_superpolyak=closure, closure_fallback=closure, superpolyak_optimizer=superpolyak_optimizer, fallback_optimizer=proxgradient_optimizer, max_inner_iter=max_inner_iter, tol = 1e-10, max_outer_iter=max_outer_iter, verbose=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.semilogy(oracle_calls, loss_list)\n",
    "plt.xlabel(\"Cumulative oracle calls\")\n",
    "plt.ylabel(\"$f(x) - f^*$\")\n",
    "plt.title(\"Lasso problem with {} parameters\".format(d*2))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}